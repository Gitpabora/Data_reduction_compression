{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZsQ1Ecdqn8Se"
      },
      "source": [
        "# CCE AIML -learners project on  data compression for Machine learning\n",
        "###by Shristi Singh, Parinita Bora\n",
        "\n",
        "\n",
        "#### The algorithms experimented are SVD, PCA, K-mean with high resolution image data\n",
        "#### This notepad is with reference to PCA for an image of cosmic object captured by James Webb Space Telescope.\n",
        "\n",
        "## 1. Introduction\n",
        "For training a machine learning model When there is large amount of  unlabeled data, unsupervised learning algorithms helps in undestanding data.\n",
        "Unsupervised learning also can help in  dimensionality reduction. \n",
        "Dimensionality reduction again can  help in data visualization (e.g. t-SNA method) \n",
        "When the data is reduced, the complexity of the model can be reduced, so as the traing time.\n",
        "\n",
        "\n",
        "## 2.  Brief review -Principal Component Analysis (PCA)\n",
        "https://en.wikipedia.org/wiki/Principal_component_analysis\n",
        "\n",
        "\n",
        "Principal Component Analysis is commonly used   dimensionality reduction method.\n",
        "The data is projected onto its orthogonal subspace.\n",
        "\n",
        "<img align=\"right\" src=\"https://upload.wikimedia.org/wikipedia/commons/f/f5/GaussianScatterPCA.svg\" width=\"400\">\n",
        "\n",
        "In the figure if the observations are in ellipsoid feature space.\n",
        "If the basis set vectors are orthogonal, highly correlated features can be removed, now the data  lies in a subspace having a smaller dimension.\n",
        "This allows reduction of space with the newer projection. Each of the ellipsoid axes with maximal dispersion is choosen.    \n",
        "\n",
        " \n",
        "\n",
        "#The mathematics behind:\n",
        "\n",
        "In order to decrease the dimensionality of our data from $n$ to $k$ with $k \\leq n$, we sort our list of axes in order of decreasing dispersion and take the top-$k$ of them.\n",
        "\n",
        "##Step 1.Calculate the covariance matrix of the data\n",
        "As per definition, covariance  the covariance of two features is : $$cov(X_i, X_j) = E[(X_i - \\mu_i) (X_j - \\mu_j)] = E[X_i X_j] - \\mu_i \\mu_j,$$ where $\\mu_i$ is the expected value of the $i$th feature. \n",
        "\n",
        "-The covariance is symmetric\n",
        "-The covariance of a vector with itself is equal to its dispersion.\n",
        "Hence the covariance matrix is symmetric with the dispersion of the corresponding features on the diagonal.\n",
        " Non-diagonal values are the covariances of the corresponding pair of features. In terms of matrices where $\\mathbf{X}$ is the matrix under observations, the covariance matrix is as follows:\n",
        "\n",
        "$$\\Sigma = E[(\\mathbf{X} - E[\\mathbf{X}]) (\\mathbf{X} - E[\\mathbf{X}])^{T}]$$\n",
        "\n",
        "##step 2.Extract the eigenvectors and the eigenvalues of that matrix\n",
        "Matrices have  eigenvalues and eigenvectors  as linear operator.  This describes a part of the  space that can  only stretch when linear operators are applied.The streching is by a corresponding value of eigen value while  the direction of Eigenvectors remaining the same.\n",
        "\n",
        "That is , a matrix $M$ with eigenvector $w_i$ and eigenvalue $\\lambda_i$ satisfies the equation : $M w_i = \\lambda_i w_i$.\n",
        "\n",
        "   \n",
        "##Step 3. Select the number of desired dimensions and filter the eigenvectors to match it, sorting them by their associated eigenvalue\n",
        "   \n",
        "The covariance matrix for $\\mathbf{X}$ is a product of $\\mathbf{X}^{T} \\mathbf{X}$. [Rayleigh quotient](https://en.wikipedia.org/wiki/Rayleigh_quotient), the maximum for sample X resides along eigenvector. Principal components  aim to keep only  the eigenvectors corresponding to the most -$k$ largest eigenvalues.\n",
        "\n",
        "## Step 4. Multiply the original space by the feature vector generated in the previous step.\n",
        "\n",
        "The matrix of the data $X$ is multipled by the components to get the projection of the data  onto the orthogonal basis for the chosen components. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P79IW2I812TW"
      },
      "source": [
        "references\n",
        "- http://stats.stackexchange.com/questions/2691/making-sense-of-principal-component-analysis-eigenvectors-eigenvalues \n",
        "-https://stats.stackexchange.com/questions/134282/relationship-between-svd-and-pca-how-to-use-svd-to-perform-pca\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/",
          "height": 55
        },
        "id": "gDdplo6XPe-n",
        "outputId": "77712f5c-52de-4eb8-943f-432ef3c5bb9d"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-2bc5bf18-9a7f-417f-bf3f-e826a31465fc\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-2bc5bf18-9a7f-417f-bf3f-e826a31465fc\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# input file downloaded online \n",
        "from google.colab import files \n",
        "file=files.upload()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_e7xEkepQUws"
      },
      "source": [
        "# The input data file \n",
        "an example data file with dimention 570X 985 x 3 is an image of Cosmic object, Captured by James Webb Space Telescope (publicly available in Nasa wesite)   "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WbRLWRW11_5z"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ryL4Ng439Wyg"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "img=plt.imread(\"/content/sample_image.jpg\")\n",
        "\n",
        "#Read and plot the image\n",
        "plt.axis(\"off\")\n",
        "plt.imshow(img)\n",
        "plt.title(\"The input image \"  )\n",
        "plt.show()\n",
        "\n",
        "#Image is a three dimensional (RGB) object.\n",
        "\n",
        "#Reshape img into a matrix\n",
        "#The array has 570 rows each of pixel 985x3. \n",
        "#Reshape it into the form of a matrix that PCA can understand. # 2955 = 985* 3\n",
        "\n",
        "img_reshaped = np.reshape(img,(570,2955 ))\n",
        "#print(img_reshaped.shape)\n",
        "\n",
        "#Make the data centred at origin\n",
        "\n",
        "img_mean=img_reshaped.mean(axis=0)\n",
        "img_reshaped=img_reshaped-img_mean\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wD1hQzZvJ3yj"
      },
      "outputs": [],
      "source": [
        "img_mean.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PSMgt_VZJ73k"
      },
      "outputs": [],
      "source": [
        "img_reshaped.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M0pv2Nk1TP0s"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "#### Apply PCA  # repeat for components 10 to 100\n",
        "\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "for components in range( 10,110, 10):\n",
        "  #components=10\n",
        "  pca=PCA(n_components=components)\n",
        "  reduced=pca.fit(img_reshaped)\n",
        "\n",
        "  #Note that T=XW, where W is the weight matrix with each column representing the eigen vector. Also WW'=I because W is an orthogonal coordinate system and eigen vectors are normalized to unit length.\n",
        "  #To get back from T coordinate system to W coordinate system one needs to do as follows, X=TW'. Note that if W is the complete matrix then X is the exact representation, otherwise not. Also remember to do the de-normalization of X to get to the original values.\n",
        "\n",
        "  #Following represents the values in the new coordinate system T=XW\n",
        "  img_transformed_coordinate=pca.transform(img_reshaped)\n",
        "  print(img_transformed_coordinate.shape)\n",
        "\n",
        "############  Explained variance \n",
        "  print(np.sum(pca.explained_variance_ratio_) )\n",
        "  plt.grid()\n",
        "  fname_ev =  \"Explained_variance\" + str(components)\n",
        "  #plt.savefig(fname=fname_ev)\n",
        "  plt.title(\"pca Explained variance for components =%d\" %components )\n",
        "  plt.plot(np.cumsum(pca.explained_variance_ratio_ * 100))\n",
        "  plt.xlabel('Number of components')\n",
        "  plt.ylabel('Explained variance')\n",
        "  plt.savefig(fname=fname_ev)\n",
        "  plt.show()\n",
        "#############  Reconstuction\n",
        "  #To go back to the old coordinate systesm, either use the inbuilt command or do the operations manually X=TW'\n",
        "  #img_original_coordinate = pca.inverse_transform(img_transformed_coordinate)\n",
        "\n",
        "  img_original_coordinate = img_transformed_coordinate.dot(pca.components_)\n",
        "\n",
        "  #Shifting the mean to original values\n",
        "\n",
        "  img_original_coordinate = img_original_coordinate+img_mean\n",
        "\n",
        "  #Reshaping the matrix \n",
        "\n",
        "  img_final=np.reshape(img_original_coordinate,(570,985,3))\n",
        "  img_final=img_final.astype('int')\n",
        "\n",
        "  img_final[img_final<0]=0\n",
        "  img_final[img_final>255]=255\n",
        "  \n",
        "  plt.title(\" Reconstructed Image from components =%d\" %components )\n",
        "  plt.axis(\"off\")\n",
        "  plt.imshow(img_final)\n",
        "  #plt.savefig(\"imagedata_new.jpg\")\n",
        "  #plt.show()\n",
        "  fname_r =  \"reconstructed\" + str(components)\n",
        "  plt.savefig(fname=fname_r, dpi=100)\n",
        "  plt.show()\n",
        "################ Reduction \n",
        "  reduced_s = []\n",
        "  for i in range(img_reshaped.shape[1]):\n",
        "      N_ = i\n",
        "      totsize = img_reshaped.shape[0]*img_reshaped.shape[1]\n",
        "      redsize  = img_reshaped.shape[0]*N_ + N_*N_ + N_*img_reshaped.shape[1];\n",
        "      reduced_s.append((totsize- redsize)/totsize * 100 )\n",
        "\n",
        "  plt.title(\" Percentage Reduction in Image Size for components =%d\" %components )\n",
        "  plt.grid()\n",
        "  plt.ylabel(\"Percentage Reduction in Size\")\n",
        "  plt.xlabel(\" Number of Principal Components\")\n",
        "  plt.plot(reduced_s)\n",
        "  fname_pcr =  \"Percentage_reduction\" + str(components)\n",
        "  plt.savefig(fname=fname_pcr, dpi=100)\n",
        "  plt.show()\n",
        "########Data Compression\n",
        "\n",
        "# Data Compression Achieved\n",
        "#Number of values required to store the original image\n",
        "\n",
        "  original_number_of_values=570*985*3\n",
        "\n",
        "#Number of values required to store the original image\n",
        "\n",
        "  new_number_of_values=570*components+985*components+components\n",
        "  space_required_in_percentage= ((original_number_of_values-new_number_of_values)/original_number_of_values)*100\n",
        "\n",
        "  print(\"The compression Ratio for components= %d\", components)\n",
        "  print(\"%2f\" % space_required_in_percentage  )\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hdu4FHKtn8Sw"
      },
      "source": [
        "# Data compression"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e0L5GL2eUUyS"
      },
      "source": [
        "##calculate compression ratio for components [10, 20,30,40, 50,60,70,80,90,100] - to be updated in  code \n",
        "\n",
        "    | components  | compression ratio  |\n",
        "    |------------ | ------------------ | \n",
        "    | 10          |  99.076202         | \n",
        "    | 20          |  98.152403         |\n",
        "    | 30          |  97.228605         |\n",
        "    | 40          |  96.304806         |\n",
        "    | 50          |  95.381008         |\n",
        "    | 60          |  94.457209         |\n",
        "    | 70          |  93.533411         |\n",
        "    | 80          |  92.609612         |\n",
        "    | 90          |  91.685814         |\n",
        "    | 100         |  90.762015         |"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bK-MndpufNnD"
      },
      "outputs": [],
      "source": [
        "!ls -ltr"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nj3s8mq8fN1j"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3VAHojrHn8T1"
      },
      "source": [
        "references\n",
        "- [Q&A](http://stats.stackexchange.com/questions/2691/making-sense-of-principal-component-analysis-eigenvectors-eigenvalues) \n",
        "-https://stats.stackexchange.com/questions/134282/relationship-between-svd-and-pca-how-to-use-svd-to-perform-pca\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}